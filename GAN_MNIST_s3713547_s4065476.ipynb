{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GAN_MNIST.ipynb","provenance":[{"file_id":"10kmFSZx2mP3bT2ZvJ55rpfdeAcPVGz6z","timestamp":1589117741427},{"file_id":"1LyO4pTTpzAWtt_v882qar4CtGK1hMWkQ","timestamp":1588568030656}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"FJWH7LUfR3UR","colab_type":"code","colab":{}},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v5UEcazVR4WN","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rFHUllv0WOX","colab_type":"code","colab":{}},"source":["from keras.datasets import mnist\n","from keras.layers import Input, Dense, Reshape, Flatten\n","from keras.layers import BatchNormalization\n","from keras.layers.advanced_activations import LeakyReLU, ReLU\n","from keras.models import Sequential, Model\n","from tensorflow.keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","import numpy as np\n","class MNISTGAN():\n","  def __init__(self):\n","    self.img_rows = 28\n","    self.img_cols = 28\n","    self.channels = 1\n","    self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","    self.latent_dim = 64\n","    optimizer = Adam(0.0001, 0.5)\n","    self.discriminator = self.build_discriminator()\n","    self.discriminator.compile(loss='binary_crossentropy',\n","                                optimizer=Adam(0.0002, 0.5),\n","                                metrics=['accuracy'])\n","    self.generator = self.build_generator()\n","    z = Input(shape=(self.latent_dim,))\n","    img = self.generator(z)\n","    self.discriminator.trainable = False\n","    validity = self.discriminator(img)\n","    self.combined = Model(z, validity)\n","    self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n","\n","  def build_generator(self):\n","      model = Sequential()\n","      model.add(Dense(256, input_dim=self.latent_dim))\n","      model.add(LeakyReLU(alpha=0.2))\n","      model.add(BatchNormalization(momentum=0.8))\n","      model.add(Dense(512))\n","      model.add(LeakyReLU(alpha=0.2))\n","      model.add(BatchNormalization(momentum=0.8))\n","      model.add(Dense(1024))\n","      model.add(LeakyReLU(alpha=0.2))\n","      model.add(BatchNormalization(momentum=0.8))\n","      model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n","      model.add(Reshape(self.img_shape))\n","      noise = Input(shape=(self.latent_dim,))\n","      img = model(noise)\n","      model.summary()\n","      return Model(noise, img)\n","\n","  def build_discriminator(self):\n","      model = Sequential()\n","      model.add(Flatten(input_shape=self.img_shape))\n","      model.add(Dense(512))\n","      model.add(LeakyReLU(alpha=0.2))\n","      model.add(Dense(256))\n","      model.add(LeakyReLU(alpha=0.2))\n","      model.add(Dense(1, activation='sigmoid'))\n","      img = Input(shape=self.img_shape)\n","      validity = model(img)\n","      model.summary()\n","      return Model(img, validity)\n","\n","  def train(self, epochs, batch_size=128, sample_interval=1000):\n","      (X_train, _), (_, _) = mnist.load_data()\n","      X_train = X_train / 127.5 - 1.\n","      X_train = np.expand_dims(X_train, axis=3)\n","      valid = np.ones((batch_size, 1))\n","      fake = np.zeros((batch_size, 1))\n","      for epoch in range(epochs):\n","          idx = np.random.randint(0, X_train.shape[0], batch_size)\n","          imgs = X_train[idx]\n","          noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","          g_loss = self.combined.train_on_batch(noise, valid)\n","          gen_imgs = self.generator.predict(noise)\n","          d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n","          d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n","          d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","          print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n","          if epoch % sample_interval == 0:\n","            self.generate_numbers(epoch)\n","\t\t\t\n","  def generate_numbers(self, epoch):\n","      noise = np.random.normal(0, 1, (1, gan.latent_dim))\n","      predictions = gan.generator.predict(noise)\n","      print(predictions.shape)\n","      print(\"Reshaping\")\n","      generated_seq = np.reshape(predictions[0], (28,28))\n","      print(generated_seq.shape)\n","      plt.figure(figsize = (5,5))\n","      plt.imshow(generated_seq,aspect='auto',cmap='gray')\n","      #Path to be created in Google Drive to avoid failure\n","      plt.savefig(\"/content/drive/My Drive/DL/GAN/Results/GAN_MNIST/%d.png\" % epoch)\n","      plt.show()\n","      plt.close()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WSDQZxHU0xxI","colab_type":"code","colab":{}},"source":["gan = MNISTGAN()\n","gan.train(epochs=50000, batch_size=32, sample_interval=1000)"],"execution_count":0,"outputs":[]}]}